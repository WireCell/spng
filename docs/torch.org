#+title: Notes on use of Torch.

This collects some notes on the use of the C++ API to PyTorch.

* Names

Rather confusingly:

- PyTorch names the project at https://pytorch.org
- This package [[https://stackoverflow.com/a/52708294][derives from Torch7]] 
- ~torch~ names the Python API / package.
- LibTorch names the C++ API, provided alone or with the ~torch~ Python package.
- ~py-torch~ is the Spack package name for the Python ~torch~ package
- ~pytorch~ is the name of the WCT subpackage that houses LibTorch-based DNN inference and FFT.
- ~LIBTORCH~ is the WCT ~wcb~ environment label for building against a PyTorch installation.
- ~--with--libtorch-*~ are WCT ~wcb~ command line args (which may point into the Python ~torch~ install area).

WCT [[https://github.com/WireCell/wire-cell-toolkit/issues/343][Issue 343]] calls attention to this naming confusion and suggests using the name "torch" everywhere.  Here, we assume "torch" generically and state Python or C++ where it is not obvious.

* Install and build problems

** Include directories

Using a few code distributions:

- ~pip install torch~
- ~libtorch-cxx11-abi-shared-with-deps-2.4.1+cu124.zip~
- Spack build of ~py-torch-2.4.0~

One would think an include location (~-I~) of this directory is needed:

#+begin_example
$TORCH_PREFIX/include
#+end_example

That fails to find the main entry header ~torch/torch.h~.  That and other important headers are in an unexpected, deeper directory:

#+begin_example
$TORCH_PREFIX/include/torch/csrc/api/include/torch/torch.h
#+end_example

Using *this* as a ~-I~ also fails as the first directory includes required headers included like ~#include <torch/HEADER.h>~.  To make matters more confused, some headers, like ~torch/script.h~, include paths like

#+begin_example
#include <torch/csrc/api/include/torch/types.h>
#+end_example

Given both directories as ~-I~ locations allows all headers to be found by the compiler.  Apparently this is an [[https://discuss.pytorch.org/t/where-to-find-torch-torch-h/59908][intentional wart]] that is buffed smooth if using Torch's CMake support.  For waf-heads, the weirdness is exposed.  Assuming ~TORCH_PREFIX~ points to the installation base directory then this will let WCT build against it.:

#+begin_example
waf  configure [...] \
  --with-libtorch=$TORCH_PREFIX \
  --with-libtorch-include=$TORCH_PREFIX/include,$TORCH_PREFIX/include/torch/csrc/api/include
#+end_example


**  Warning about arguments to variadic macro

You may see many warnings from inside torch code like:

#+begin_example
warning: ISO C++11 requires at least one argument for the "..." in a variadic macro
#+end_example

As far as I can tell these can only be disabled by removing ~-pedantic~.

* Data context

At low level one must utilize a ~torch::Device~.  WCT ~pytorch~ defines a ~TorchContext~ which couples this device with a semaphore for cooperatively limiting resource usage of the device.  The device object is otherwise just an identifier which can be produced from a name (string).  A tensor "knows" its device and can be moved to another device.


* Tensor memory semantics

- A ~torch::Tensor~ is [[https://discuss.pytorch.org/t/tensor-move-semantics-in-c-frontend/77901/10][like a shared pointer]].

- ~torch::Tensor::detach()~ disconnects the tensor from autograd

- ~torch::Tensor::clone()~ makes a new tensor holding a copy of of the tensor.  See also ~torch::clone()~.


* Documentation

Torch is most popularly used via its Python bindings.  The C++ API is similar but the lack of popularity can make learning how to do things via internet search difficult.  The C++ documentation is fairly good.  These entry pages should be given a linear read through.  Here are some with brief topic summaries:

- [[https://pytorch.org/cppdocs/][overview]] :: understand the major parts of the API, focus on "ATen" (~at~ namespace) and "Frontend" (~torch~ namespace).  Both have ~Tensor~, the ~at::Tensor~ lacks autograd API provided by ~torch::Tensor~.

- [[https://pytorch.org/cppdocs/frontend.html][frontend]] :: The C++ "frontend" mimics the Python API 

- [[https://pytorch.org/cppdocs/notes/tensor_basics.html][tensor basics]] :: Same tensor type for CPU or CUDA tensor and may have type double, float, int.  Code for accessesing individual tensor elements differ for CPU vs CUDA.  Use ~from_blob()~ to attach tensor to existing array data.  Scalar vs zero-dimensional TEnsor.

- [[https://pytorch.org/cppdocs/api/namespace_at.html#namespace-at][=at::= namespace]] :: Low level "tensor" type and functions.

- ~torch::NoGradGuard~ :: this thread-local object turns off autograd.  It should be asserted at high level and sparingly.

* Examples

- tensor shape :: use ~tensor::sizes()~
- indexing :: see https://pytorch.org/cppdocs/notes/tensor_indexing.html must pass ~{...}~ of indices, even if 1D, this yields yet another tensor.
- accessing :: https://pytorch.org/cppdocs/notes/tensor_basics.html to get tensor elements as C++ POD 

** Random

#+begin_src c++
  // uniform 1D
  auto u1d = torch::rand({100});
  // normal 1D
  auto n1d = torch::randn({100});
  // 2D
  auto u2d = torch::rand({10, 100});  

#+end_src

** FFT

* Performance testing



#+begin_example
$ OMP_NUM_THREADS=1  ./build/spng/wcdoctest-spng -tc="spng torch convo perf*"
$ OMP_NUM_THREADS=32 ./build/spng/wcdoctest-spng -tc="spng torch convo perf*"
#+end_example

These tests:
- Does a 1-shot 2D FFT convolution and the same convo as two 1D, per-dimension FFT convolutions.
- Use array shape 1024 x 8192 powers of 2 near real detector readout sizes.
- 1000 calls to the alg a single call to ~torch::rand()~ for the initial arrays.
- Limit number of CPU threads, confirm CPU and GPU utilization with ~top~ and ~nvidia-smi~

  Results:

| threads | device | wall clock | ncalls |   CPU |  GPU | CPU mem | GPU mem |
|---------+--------+------------+--------+-------+------+---------+---------|
|      32 | GPU    | 10.4 s     |   1000 |  100% | 100% |     0.5 |     1.9 |
|       1 | GPU    | 10.4 s     |   1000 |  100% | 100% |     0.5 |     1.9 |
|      32 | CPU    | 9.47 s     |    100 | 3200% |   0% | 0.5-1.4 |       0 |
|      16 | CPU    | 11.45 s    |    100 | 1600% |   0% |         |         |
|       8 | CPU    | 16.91 s    |    100 |  800% |   0% |         |         |
|       4 | CPU    | 28.65 s    |    100 |  400% |   0% |         |         |
|       2 | CPU    | 53.07 s    |    100 |  200% |   0% |         |         |
|       1 | CPU    | 122.4      |    100 |  100% |   0% |         |         |
|       1 | CPU    | 12.3s      |     10 |  100% |   0% | 0.7-1.6 |       0 |


Observations:

- Running on GPU needs only, and all of, one CPU core.

- GPU is two orders of magnitude faster than one core and one order faster than one 32 core CPU.

- A lot of CPU memory fluctuation when running on CPU.

- No CPU mem fluctuation when running on GPU.

- GPU mem is higher than CPU mem (when running on CPU) but this may represent a high water mark.

- CPU gives less than linear scaling with number of cores.  
