#+title: Notes on use of Torch.

This collects some notes on the use of the C++ API to PyTorch.

* Names

Rather confusingly:

- PyTorch names the project at https://pytorch.org
- This package [[https://stackoverflow.com/a/52708294][derives from Torch7]] 
- ~torch~ names the Python API / package.
- LibTorch names the C++ API, provided alone or with the ~torch~ Python package.
- ~py-torch~ is the Spack package name for the Python ~torch~ package
- ~pytorch~ is the name of the WCT subpackage that houses LibTorch-based DNN inference and FFT.
- ~LIBTORCH~ is the WCT ~wcb~ environment label for building against a PyTorch installation.
- ~--with--libtorch-*~ are WCT ~wcb~ command line args (which may point into the Python ~torch~ install area).

WCT [[https://github.com/WireCell/wire-cell-toolkit/issues/343][Issue 343]] calls attention to this naming confusion and suggests using the name "torch" everywhere.  Here, we assume "torch" generically and state Python or C++ where it is not obvious.

* Install and build problems

** Include directories

Using a few code distributions:

- ~pip install torch~
- ~libtorch-cxx11-abi-shared-with-deps-2.4.1+cu124.zip~
- Spack build of ~py-torch-2.4.0~

One would think an include location (~-I~) of this directory is needed:

#+begin_example
$TORCH_PREFIX/include
#+end_example

That fails to find the main entry header ~torch/torch.h~.  That and other important headers are in an unexpected, deeper directory:

#+begin_example
$TORCH_PREFIX/include/torch/csrc/api/include/torch/torch.h
#+end_example

Using *this* as a ~-I~ also fails as the first directory includes required headers included like ~#include <torch/HEADER.h>~.  To make matters more confused, some headers, like ~torch/script.h~, include paths like

#+begin_example
#include <torch/csrc/api/include/torch/types.h>
#+end_example

Given both directories as ~-I~ locations allows all headers to be found by the compiler.  Apparently this is an [[https://discuss.pytorch.org/t/where-to-find-torch-torch-h/59908][intentional wart]] that is buffed smooth if using Torch's CMake support.  For waf-heads, the weirdness is exposed.  Assuming ~TORCH_PREFIX~ points to the installation base directory then this will let WCT build against it.:

#+begin_example
waf  configure [...] \
  --with-libtorch=$TORCH_PREFIX \
  --with-libtorch-include=$TORCH_PREFIX/include,$TORCH_PREFIX/include/torch/csrc/api/include
#+end_example


**  Warning about arguments to variadic macro

You may see many warnings from inside torch code like:

#+begin_example
warning: ISO C++11 requires at least one argument for the "..." in a variadic macro
#+end_example

As far as I can tell these can only be disabled by removing ~-pedantic~.

* Data context

At low level one must utilize a ~torch::Device~.  WCT ~pytorch~ defines a ~TorchContext~ which couples this device with a semaphore for cooperatively limiting resource usage of the device.  The device object is otherwise just an identifier which can be produced from a name (string).  A tensor "knows" its device and can be moved to another device.


* Tensor memory semantics

- A ~torch::Tensor~ is [[https://discuss.pytorch.org/t/tensor-move-semantics-in-c-frontend/77901/10][like a shared pointer]].

- ~torch::Tensor::detach()~ disconnects the tensor from autograd

- ~torch::Tensor::clone()~ makes a new tensor holding a copy of of the tensor.  See also ~torch::clone()~.


* Documentation

Torch is most popularly used via its Python bindings.  The C++ API is similar but the lack of popularity can make learning how to do things via internet search difficult.  The C++ documentation is fairly good.  These entry pages should be given a linear read through.  Here are some with brief topic summaries:

- [[https://pytorch.org/cppdocs/][overview]] :: understand the major parts of the API, focus on "ATen" (~at~ namespace) and "Frontend" (~torch~ namespace).  Both have ~Tensor~, the ~at::Tensor~ lacks autograd API provided by ~torch::Tensor~.

- [[https://pytorch.org/cppdocs/frontend.html][frontend]] :: The C++ "frontend" mimics the Python API 

- [[https://pytorch.org/cppdocs/notes/tensor_basics.html][tensor basics]] :: Same tensor type for CPU or CUDA tensor and may have type double, float, int.  Code for accessesing individual tensor elements differ for CPU vs CUDA.  Use ~from_blob()~ to attach tensor to existing array data.  Scalar vs zero-dimensional TEnsor.

- [[https://pytorch.org/cppdocs/api/namespace_at.html#namespace-at][=at::= namespace]] :: Low level "tensor" type and functions.

- ~torch::NoGradGuard~ :: this thread-local object turns off autograd.  It should be asserted at high level and sparingly.

* Examples

- tensor shape :: use ~tensor::sizes()~
- indexing :: see https://pytorch.org/cppdocs/notes/tensor_indexing.html must pass ~{...}~ of indices, even if 1D, this yields yet another tensor.
- accessing :: https://pytorch.org/cppdocs/notes/tensor_basics.html to get tensor elements as C++ POD 

** Random

#+begin_src c++
  // uniform 1D
  auto u1d = torch::rand({100});
  // normal 1D
  auto n1d = torch::randn({100});
  // 2D
  auto u2d = torch::rand({10, 100});  

#+end_src

** FFT

* Performance testing

#+begin_example
$ ./build/spng/wcdoctest-spng -t "spng torch conv"
$ OMP_NUM_THREADS=1 ./build/spng/wcdoctest-spng -t "spng torch conv"
#+end_example

That test exercises some large 1D and 2D tensor FFT and multiplications many times on CPU and GPU for a 1k x 10k array shape.  The second command limits torch to using 1 CPU core.  Testing 100k loops gives very curious results.  Listing XXXa where XXX is CPU or GPU utilization and "a" is the phase (c = tensors on CPU, g = tensors on GPU).  Utilization is checked by ~top -d1~ and ~nvidia-smi -l1~.  Time t is wall clock.  Both CPU and GPU RAM utilization is about 450 MB.

| cores |  CPUc | GPUc |  CPUg | GPUg | CPU t   | GPU t    |
|-------+-------+------+-------+------+---------+----------|
|     1 |  100% |   0% |  100% |  44% | 3930 ms | 12146 ms |
|    32 | 3200% |   0% | 3200% |  44% | 4921 ms | 12132 ms |
|-------+-------+------+-------+------+---------+----------|


